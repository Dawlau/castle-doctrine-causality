{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042a4d24",
   "metadata": {},
   "source": [
    "# TO BE OR NOT TO BE INSURED? \n",
    "## How Social Networks Influence One's Decision To Insure\n",
    "\n",
    "*Andrei Blahovici | Milena Kapralova | Luca Pantea | Paulius Skaisgiris*\n",
    "\n",
    "This project is part of the Causality course at the UvA during fall 2023. We look at an experiment that investigated how the social environment of rice farmers in rural China influences whether they adopt weather insurance (Cai, De Janvry & Sadoulet, 2015), along with other variables such as demographics or previously adopting weather insurance.\n",
    "\n",
    "\n",
    "TODO: make the image smaller using [rice.png](attachment:rice.png) or make it appear using\n",
    "<img src=\"attachment:rice.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02250d5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3732e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING:\n",
    "# The installation takes a few minutes.\n",
    "# Only run during the first time running this notebook and if you don't have these packages installed.\n",
    "# Run in terminal command line instead if it does not work.\n",
    "\n",
    "# !pip install hyppo\n",
    "# !pip install pingouin\n",
    "# !pip install conditional_independence\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7972eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulius/.conda/envs/causal_data_science_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import conditional_independence\n",
    "import hyppo\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pingouin as pg\n",
    "from sklearn import svm\n",
    "from IPython.display import Image, display\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import chain, combinations\n",
    "from statsmodels.datasets import utils as du\n",
    "from itertools import permutations\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "# DoWhy\n",
    "import dowhy\n",
    "import dowhy.datasets\n",
    "from dowhy import CausalModel\n",
    "from dowhy.causal_identifier import backdoor\n",
    "\n",
    "# Hide some warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configs\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6e8fc",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- [ ] Linear correlation of variables we took (heatmap viz)\n",
    "- [x] Does the causal graph check out given the correlation and independence tests?\n",
    "- [ ] Introduction to the project and dataset\n",
    "- [x] Why did we choose these specific variables?\n",
    "- [ ] Backdoor criterion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6924776d",
   "metadata": {},
   "source": [
    "## 1 Introduction and Motivation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Description of the Dataset\n",
    "- **Paper introducing the dataset**: [Cai, Jing, Alain De Janvry, and Elisabeth Sadoulet. 2015. “Social Networks and the Decision to Insure.” ](https://www.aeaweb.org/articles?id=10.1257/app.20130442)\n",
    "- **Data Source**: Data from a randomized experiment in rural China, focusing on weather insurance adoption among rice farmers. You can find the dataset in [this repository](https://github.com/NickCH-K/causaldata/tree/main/Python/causaldata/social-insure).\n",
    "- **Observational Data**: Includes administrative records of insurance purchases and surveys on social networks, demographics, rice production, income, natural disasters, risk attitudes, and future disaster perceptions.\n",
    "- **Interventional Data**: The experiment involved providing intensive information sessions about weather insurance to a subset of farmers, generating data on the impact of information dissemination through social networks on insurance uptake.\n",
    "- **Collection Method**: Data collected through administrative records from the People's Insurance Company of China (PICC) and two surveys - a social network survey pre-experiment and a household survey post-insurance decision.\n",
    "\n",
    "### Causal Questions Investigated\n",
    "- **Primary Investigation**: Understanding the influence of social networks on the decision to purchase weather insurance.\n",
    "- **Specific Questions**:\n",
    "  1. Does providing intensive information to a subset of farmers increase insurance uptake among their social networks?\n",
    "  2. Mechanisms of influence - is it through diffusion of insurance knowledge or observation of others' purchase decisions?\n",
    "\n",
    "### Assumptions of the Dataset\n",
    "- **Causal Sufficiency**: Assumes no unmeasured confounding variables affecting both network structure and insurance adoption.\n",
    "- **No Cycles in the Causal Graph**: Assumes a linear progression from information dissemination to changes in insurance adoption, without feedback loops influencing initial information distribution.\n",
    "- **Positivity**: Every farmer had a non-zero probability of being in both treatment and control conditions due to the randomized nature of the experiment.\n",
    "- **SUTVA (Stable Unit Treatment Value Assumption)**: Assumes the treatment (information session) for one farmer does not directly affect another farmer's outcome not receiving the treatment, except through defined social networks.\n",
    "- **Randomization**: Ensures unbiased estimates of treatment effects by randomly assigning farmers to different types of information sessions.\n",
    "\n",
    "This study provides key insights into the role of social networks in economic decisions, especially in contexts with complex products like weather insurance in rural areas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- Introduce the datasets, the assumptions and the causal questions you are investigating.\n",
    "\n",
    "• Describe your dataset (e.g. what are the observational data and how they were collected, in case there are interventional data, also what are they and how they were collected).\n",
    "\n",
    "• Describe the causal questions you wish to answer (e.g. “we investigate the effect of X on Y”).\n",
    "\n",
    "• Describe the assumptions of your dataset (causal sufficiency, no cycles in the causal graph, positivity, etc). -->\n",
    "\n",
    "\n",
    "<!-- <img src=https://d3i71xaburhd42.cloudfront.net/766441c1ab7f4390a5a8c0fa05ec9dc3cd4854d1/58-Figure1-1.png \n",
    "     align=\"center\" \n",
    "     width=\"500\" />\n",
    "     \n",
    "*Source: Shields (2016)* -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6d98e",
   "metadata": {},
   "source": [
    "The following is a brief explanation of the dataset taken from [here](https://github.com/NickCH-K/causaldata/blob/main/Python/causaldata/social_insure/data.py):\n",
    "- **Number of observations:** 1410\n",
    "- **Number of variables** - 13\n",
    "- Variable explanations:\n",
    "    - *address* - Natural village\n",
    "    - *village* - Administrative village\n",
    "    - *takeup_survey* - Whether farmer ended up purchasing insurance. (1 = yes)\n",
    "    - *age* - Household Characteristics - Age\n",
    "    - *agpop* - Household Characteristics - Household Size\n",
    "    - *ricearea_2010* - Area of Rice Production\n",
    "    - *disaster_prob* - Perceived Probability of Disasters Next Year\n",
    "    - *male* - Household Caracteristics: Gender of Household Head (1 = male)\n",
    "    - *default* - \"Default option\" in experimental format assigned to. (1 = default is to buy, 0 = default is to not buy)\n",
    "    - *intensive* - Whether or not was assigned to \"intensive\" experimental session (1 = yes)\n",
    "    - *risk_averse* - Risk aversion measurement\n",
    "    - *literacy* - 1 = literate, 0 = illiterate\n",
    "    - *pre_takeup_rate* - Takeup rate prior to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd58be",
   "metadata": {},
   "source": [
    "### Removing variables for causal analysis\n",
    "\n",
    "After reading the dataset's paper, we chose not to adopt the authors' approach, as it focused on the impact of the effect of social networks on the `takeup_survey`` variable. We did not learn sufficient tools in the course to suitably tackle such questions. Additionally, the dataset's 13 variables were excessive. Therefore, we decided to eliminate some features and conduct a causal analysis on the remaining ones using methods learned in class. The following is a list of discussion for each variable and why we decided to (not) keep it:\n",
    "- *address* - **do not keep**: geographical feature, relevant only for the social network analysis.\n",
    "- *village* - **do not keep**: geographical feature, relevant only for the social network analysis.\n",
    "- *takeup_survey* - **keep**: control variable.\n",
    "- *age* - **keep**: we hypothesize that the person's experience is a strong feature that may impact other variables such as whether to buy insurance.\n",
    "- *agpop* - **keep**: we think a household size may affect the person's decision to get insurance.\n",
    "- *ricearea_2010* - **keep**: this seems to be a variable representing some kind of financial situation of the household, seems reasonable to be relevant for whether insurance is bought\n",
    "- *disaster_prob* - **do not keep**: Because we decide to keep `risk_averse` instead which seems very similar. Quote from the dataset's paper: \"The perceived probability of future disasters was elicited by asking, \"What do you think is the probability of a disaster that leads to more than 30% loss in yield next year?\".\n",
    "- *male* - **do not keep**: we see no reason why gender may affect purchasing insurance.\n",
    "- *default* - **do not keep**: very similar to `pre_takeup_rate`.\n",
    "- *intensive* - **do not keep**: feature relevant only for the social network analysis.\n",
    "- *risk_averse* - **keep**: Similar feature to `disaster_prob`. Quote for the dataset's paper: \"Risk attitudes were elicited by asking households to choose between a certain amount with increasing values of 50, 80, 100, 120, and 150 RMB (riskless option A), and risky gambles of (200RMB, 0) with probability (0.5, 0.5) (risky option B). The proportion of riskless options chosen was then used as a measure of risk aversion, which ranges from 0 to 1.\"\n",
    "- *literacy* - **keep**: a feature loosely indicating the education level of an individual. May be interesting to investigate how literacy contributes to buying insurance - a somewhat abstract promise for a future safety in case of disaster.\n",
    "- *pre_takeup_rate* - **keep**: we expect this feature to have a large effect on `takeup_survey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3248b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.load_csv(\"rice-insurance-causality\", os.path.join(\"data\", \"Cai_2015.csv\"), sep=\",\")\n",
    "data = du.strip_column_names(data)\n",
    "data = data.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3762d09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>village</th>\n",
       "      <th>takeup_survey</th>\n",
       "      <th>age</th>\n",
       "      <th>agpop</th>\n",
       "      <th>ricearea_2010</th>\n",
       "      <th>disaster_prob</th>\n",
       "      <th>male</th>\n",
       "      <th>default</th>\n",
       "      <th>intensive</th>\n",
       "      <th>risk_averse</th>\n",
       "      <th>literacy</th>\n",
       "      <th>pre_takeup_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beilian2</td>\n",
       "      <td>beilian</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beilian2</td>\n",
       "      <td>beilian</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beilian2</td>\n",
       "      <td>beilian</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beilian2</td>\n",
       "      <td>beilian</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beilian2</td>\n",
       "      <td>beilian</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    address  village  takeup_survey   age  agpop  ricearea_2010  \\\n",
       "0  beilian2  beilian              0  62.0    2.0           10.0   \n",
       "1  beilian2  beilian              1  63.0    5.0           15.0   \n",
       "2  beilian2  beilian              1  44.0    3.0            7.5   \n",
       "4  beilian2  beilian              0  52.0    6.0           11.0   \n",
       "5  beilian2  beilian              1  53.0    5.0           35.0   \n",
       "\n",
       "   disaster_prob  male  default  intensive  risk_averse  literacy  \\\n",
       "0           30.0   1.0        1          0          0.0       0.0   \n",
       "1          100.0   1.0        1          0          0.0       1.0   \n",
       "2           20.0   1.0        1          1          0.0       1.0   \n",
       "4            0.0   1.0        1          1          0.2       1.0   \n",
       "5            0.0   1.0        1          0          0.0       1.0   \n",
       "\n",
       "   pre_takeup_rate  \n",
       "0         0.071429  \n",
       "1         0.071429  \n",
       "2         0.071429  \n",
       "4         0.071429  \n",
       "5         0.071429  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "066962a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'village',\n",
       " 'takeup_survey',\n",
       " 'age',\n",
       " 'agpop',\n",
       " 'ricearea_2010',\n",
       " 'disaster_prob',\n",
       " 'male',\n",
       " 'default',\n",
       " 'intensive',\n",
       " 'risk_averse',\n",
       " 'literacy',\n",
       " 'pre_takeup_rate']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e56bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_variables = [\"age\", \"agpop\", \"ricearea_2010\", \"risk_averse\", \"literacy\", \"pre_takeup_rate\"]\n",
    "dependent_variables = [\"takeup_survey\"]\n",
    "\n",
    "data = data.drop(data.columns.difference(control_variables + dependent_variables), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78beec69",
   "metadata": {},
   "source": [
    "## TODO: why did we choose these specific features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb515b",
   "metadata": {},
   "source": [
    "## 2 Exploratory Data Analysis\n",
    "\n",
    "As shown in Tutorial 2. \n",
    "\n",
    "• Testing correlation / dependence for the variables in the dataset and show how they are dependent.\n",
    "\n",
    "• Discuss the true causal graph of the dataset, if it’s known, and otherwise discuss a reasonable guess.\n",
    "\n",
    "\n",
    "**Note form the 5th tutorial:\n",
    "If we see that our relationships between variables is nonlinear, we can try to transform them to become linear. Alternatively, we could use nonparametric tests for dependence (as opposed to the naive, linear gaussian tests) but they require more data (maybe we could try them with 500 samples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbace17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationManager:\n",
    "    '''\n",
    "    Takes care of exploratory analyses, including d-separation, visualisation and testing for independences.\n",
    "    '''\n",
    "    def __init__(self, data, G=None):\n",
    "        '''\n",
    "        :param data: data (df)\n",
    "        :param G: the graph (a DiGraph object)\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.G = G\n",
    "        \n",
    "    def is_d_separated(self, x, y, z):\n",
    "        '''\n",
    "        Verifies whether two (sets of) variables are d-separated by a (set) of variables.\n",
    "        \n",
    "        :param x: a set of independent variable(s), len(x) > 0\n",
    "        :param y: a set of dependent variable(s), len(y) > 0\n",
    "        :param z: a set of conditioning variables, len(z) >= 0\n",
    "        '''\n",
    "        return nx.algorithms.d_separated(G=self.G, x=x, y=y, z=z)\n",
    "\n",
    "    def visualize_rel(self, x, y):\n",
    "        '''\n",
    "        Visualizes the relationship between x and y.\n",
    "\n",
    "        :param x: the independent variable\n",
    "        :param y: the dependent variable\n",
    "        '''\n",
    "        plt.scatter(self.data[x], self.data[y])\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.title(f'Data distribution of {x} and {y}')\n",
    "        plt.show()\n",
    "    \n",
    "    def is_dependent(self, x, y, z=[]):\n",
    "        '''\n",
    "        Tests  whether two variables are dependent. \n",
    "        By default tests for marginal dependence, if z (conditioning set) \n",
    "        is specified, tests for conditional dependence.\n",
    "        \n",
    "        :param x: an independent variable (str)\n",
    "        :param y: a dependent variable (str)\n",
    "        :param z: conditioning set (list)\n",
    "        \n",
    "        Returns n, r, 95% CI and a p-value (df).\n",
    "        '''\n",
    "        return pg.partial_corr(data=self.data, x=x, y=y, covar=z, method='pearson')\n",
    "\n",
    "    def is_marginally_dependent(self, x, y):\n",
    "        '''\n",
    "        Tests  whether two variables are marginally dependent. \n",
    "        \n",
    "        :param x: an independent variable (list/array)\n",
    "        :param y: a dependent variable (list/array)\n",
    "        \n",
    "        Returns n, r, 95% CI and a p-value, BF10 and power (df).\n",
    "        '''\n",
    "        return pg.corr(x=x, y=y, method='pearson')\n",
    "    \n",
    "    def is_hsic_dependent(self, x, y):\n",
    "        '''\n",
    "        Tests the dependece of two variables using the Hilbert Schmidt Independence Criterion.\n",
    "        \n",
    "        :param x: an independent variable (list/array)\n",
    "        :param y: a dependent variable (list/array)\n",
    "        \n",
    "        Returns the hsic statistic and p-value (tuple).\n",
    "        '''\n",
    "        hsic, p = hyppo.independence.Hsic().test(self.data[x], self.data[y])\n",
    "        return hsic, p\n",
    "    \n",
    "    def test_all(self, variables, method='marginal'):\n",
    "        '''\n",
    "        Tests dependence of all possible permutations of variables specified.\n",
    "        By default tests for marginal dependence, if the method variable is changed,\n",
    "        tests for conditional dependence.\n",
    "        \n",
    "        :param variables: all variables to consider (list)\n",
    "        :param method: {'marginal', 'conditional', 'both'}\n",
    "        \n",
    "        Returns a dictionary of p-values.\n",
    "        '''\n",
    "        dependence_tests = {}\n",
    "        \n",
    "        if method in ['marginal', 'both']:\n",
    "            for var1, var2 in permutations(variables, 2):\n",
    "                dependence_tests[var1, var2] = pg.partial_corr(data=self.data, x=var1, y=var2, covar=[], method='pearson')['p-val'].item()\n",
    "                \n",
    "        if method in ['conditional', 'both']:\n",
    "            for var1, var2, cond in permutations(variables, 3):\n",
    "                 dependence_tests[var1, var2, cond] = pg.partial_corr(data=self.data, x=var1, y=var2, covar=[cond], method='pearson')['p-val'].item()\n",
    "        \n",
    "        return dependence_tests\n",
    "    \n",
    "    def test_all_hsic(self, variables, method=\"marginal\"):\n",
    "        '''\n",
    "        Tests dependence of all possible permutations of variables specified using HSIC.\n",
    "        \n",
    "        :param variables: all variables to consider (list)\n",
    "        \n",
    "        Returns a dictionary of p-values.\n",
    "        '''\n",
    "        dependence_tests = {}\n",
    "\n",
    "        if method in [\"marginal\", \"both\"]:\n",
    "            for var1, var2 in permutations(variables, 2):\n",
    "                dependence_tests[var1, var2] = conditional_independence.hsic_test(np.stack((self.data[var1], self.data[var2]), axis=1), i=0, j=1, cond_set=[])[\"p_value\"]\n",
    "        \n",
    "        if method in [\"conditional\", \"both\"]:\n",
    "            for var1, var2, cond in permutations(variables, 3):\n",
    "                dependence_tests[var1, var2, cond] = conditional_independence.hsic_test(np.stack((self.data[var1], self.data[var2], self.data[cond]), axis=1), i=0, j=2, cond_set=[2])[\"p_value\"]\n",
    "                \n",
    "        return dependence_tests\n",
    "\n",
    "exploration_manager = ExplorationManager(data)\n",
    "\n",
    "# there is 0 linear correlation between the variables\n",
    "# for x, y in itertools.combinations(control_variables + dependent_variables, 2):\n",
    "#     exploration_manager.visualize_rel(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dependency_tests = exploration_manager.test_all_hsic(control_variables + dependent_variables, method=\"marginal\")\n",
    "dependency_tests_cond = exploration_manager.test_all_hsic(control_variables + dependent_variables, method=\"conditional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a167cb59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_tests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Remove NaNs as it's unclear how to deal with them (re-running the tests doesn't really work sometimes)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dependency_tests \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dependency_tests\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misnan(v)}\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m (var1, var2), v \u001b[39min\u001b[39;00m dependency_tests\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m v \u001b[39m>\u001b[39m alpha:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_tests' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "# Remove NaNs as it's unclear how to deal with them (re-running the tests doesn't really work sometimes)\n",
    "dependency_tests = {k: v for k, v in dependency_tests.items() if not np.isnan(v)}\n",
    "\n",
    "# > alpha => independent\n",
    "for (var1, var2), v in dependency_tests.items():\n",
    "    if v > alpha:\n",
    "        print(var1, \"NOT dep\", var2)\n",
    "    else:\n",
    "        print(var1, \"dep\", var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age _/|_ agpop | ricearea_2010\n",
      "age _/|_ agpop | pre_takeup_rate\n",
      "age _/|_ ricearea_2010 | agpop\n",
      "age _/|_ ricearea_2010 | pre_takeup_rate\n",
      "age _/|_ literacy | agpop\n",
      "age _/|_ literacy | ricearea_2010\n",
      "age _/|_ literacy | pre_takeup_rate\n",
      "agpop _/|_ age | literacy\n",
      "agpop dep ricearea_2010 | age\n",
      "agpop _/|_ ricearea_2010 | literacy\n",
      "agpop dep literacy | age\n",
      "agpop _/|_ literacy | ricearea_2010\n",
      "ricearea_2010 _/|_ age | risk_averse\n",
      "ricearea_2010 _/|_ agpop | risk_averse\n",
      "ricearea_2010 dep risk_averse | age\n",
      "ricearea_2010 _/|_ risk_averse | agpop\n",
      "ricearea_2010 dep literacy | age\n",
      "ricearea_2010 _/|_ literacy | agpop\n",
      "ricearea_2010 _/|_ literacy | risk_averse\n",
      "ricearea_2010 dep takeup_survey | age\n",
      "ricearea_2010 _/|_ takeup_survey | agpop\n",
      "ricearea_2010 _/|_ takeup_survey | risk_averse\n",
      "risk_averse _/|_ ricearea_2010 | literacy\n",
      "risk_averse _/|_ literacy | age\n",
      "risk_averse _/|_ literacy | ricearea_2010\n",
      "risk_averse _/|_ takeup_survey | age\n",
      "risk_averse _/|_ takeup_survey | ricearea_2010\n",
      "risk_averse _/|_ takeup_survey | literacy\n",
      "pre_takeup_rate _/|_ takeup_survey | age\n",
      "pre_takeup_rate _/|_ takeup_survey | ricearea_2010\n",
      "takeup_survey _/|_ risk_averse | agpop\n",
      "takeup_survey _/|_ pre_takeup_rate | agpop\n"
     ]
    }
   ],
   "source": [
    "# Remove NaNs as it's unclear how to deal with them (re-running the tests doesn't really work sometimes)\n",
    "dependency_tests_cond = {k: v for k, v in dependency_tests_cond.items() if not np.isnan(v)}\n",
    "\n",
    "# > alpha => independent\n",
    "seen = {}\n",
    "for (var1, var2, cond), v in dependency_tests_cond.items():\n",
    "    if v > alpha and (var1, var2) in dependency_tests and dependency_tests[var1, var2] <= alpha and ((var1, var2, cond) not in seen and (var2, var1, cond) not in seen):\n",
    "        print(var1, \"NOT dep\", var2, \"|\", cond)\n",
    "\n",
    "    if v <= alpha and (var1, var2) in dependency_tests and dependency_tests[var1, var2] <= alpha and ((var1, var2, cond) not in seen and (var2, var1, cond) not in seen):\n",
    "        print(var1, \"dep\", var2, \"|\", cond)\n",
    "\n",
    "    seen[var1, var2, cond] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5065084",
   "metadata": {},
   "source": [
    "Solely given the (conditional) independence tests above, we arrive at the following graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0189523",
   "metadata": {},
   "source": [
    "![graph-before-common-sense](graph-before-common-sense.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b2e57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d607df40",
   "metadata": {},
   "source": [
    "![graph-after-common-sense](graph-after-common-sense.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b2e7c",
   "metadata": {},
   "source": [
    "## 3 Identifying Estimands\n",
    "\n",
    "As shown in Tutorials 3 and 4. Identify possible adjustment sets by hand by using:\n",
    "\n",
    "• Backdoor criterion (most important)\n",
    "\n",
    "• Frontdoor criterion\n",
    "\n",
    "• Instrumental variables\n",
    "\n",
    "Report what happens for these methods even if they don’t apply and explain why. Also show the results you get for each of these estimands from doWhy and compare with the ones you found by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93baeb",
   "metadata": {},
   "source": [
    "### Backdoor criterion (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorManager:\n",
    "    '''\n",
    "    This class takes care of the backdoor adjustment.\n",
    "    '''\n",
    "    def __init__(self, G, node_x, node_y):\n",
    "        '''\n",
    "        :param G: graph (a DiGraph object)\n",
    "        :param node_x: a node whose effect we are trying to predict\n",
    "        :param node_y: a node effect on which we are trying to predict\n",
    "        '''\n",
    "        self.G = G\n",
    "        self.node_x = node_x\n",
    "        self.node_y = node_y\n",
    "        self.descendants_node_x = nx.descendants(self.G, node_x) | {node_x}\n",
    "        \n",
    "    def draw(self, pos, edge_color='black'):\n",
    "        '''\n",
    "        Draws the graph given a certain position and color of the nodes.\n",
    "        '''\n",
    "        nx.draw(self.G, pos=pos, with_labels=True, node_size=500, node_color='w', edgecolors='black', edge_color=edge_color)\n",
    "        \n",
    "    def write_gml(self, fname='backdoor_criterion_graph.gml'):\n",
    "        nx.write_gml(G, fname)\n",
    "        \n",
    "    def get_all_paths(self):\n",
    "        H = self.G.to_undirected()\n",
    "        all_paths = list(nx.all_simple_paths(H, self.node_x, self.node_y))\n",
    "        return all_paths\n",
    "    \n",
    "    def get_backdoor_paths(self):\n",
    "        bd = backdoor.Backdoor(self.G, self.node_x, self.node_y)\n",
    "        all_paths = self.get_all_paths()\n",
    "        backdoor_paths = [path for path in all_paths if bd.is_backdoor(path)]\n",
    "        return backdoor_paths\n",
    "        \n",
    "    def give_coll_noncoll_on_(self, path):\n",
    "        '''\n",
    "        Finds all colliders and non-colliders on a path.\n",
    "        '''\n",
    "        colliders = np.array([])\n",
    "        non_colliders = []\n",
    "        path_len = len(path)\n",
    "        \n",
    "        # Collider\n",
    "        ## Loop through adjacent variables on the path, ignore the source and target variables as potential colliders\n",
    "        for node0, node1, node2 in zip(path[0:path_len-2], path[1:path_len-1], path[2:]):\n",
    "            if self.G.has_edge(node0, node1) and self.G.has_edge(node2, node1):\n",
    "                ## Add the collider (and all its descendants) to the list\n",
    "                colliders = np.append(colliders, list(nx.descendants(self.G,node1)) + [node1])\n",
    "        colliders = colliders.flatten()\n",
    "        \n",
    "        # Non-collider\n",
    "        non_colliders = [x for x in path[1:-1] if x not in colliders]\n",
    "\n",
    "        return colliders, non_colliders\n",
    "    \n",
    "    def find_adjustment_variables(self):\n",
    "        '''\n",
    "        Performs the backdoor criterion search.\n",
    "        '''\n",
    "        self.adjustment_variables = pd.DataFrame(columns=['path', 'colliders', 'non_colliders'])\n",
    "        paths = self.get_backdoor_paths()\n",
    "        \n",
    "        for path in paths:\n",
    "            colliders, non_colliders = self.give_coll_noncoll_on_(path)\n",
    "            self.adjustment_variables.loc[len(self.adjustment_variables.index)] = [path, colliders, non_colliders]\n",
    "    \n",
    "    def find_adjustment_sets(self, method='default'):\n",
    "        '''\n",
    "        Finds backdoor adjustment sets based on the adjustment variables and method. \n",
    "        Default method finds all the minimum-sized and maximum-sized adjustment sets, \n",
    "        \n",
    "        :param method_name: {'default', 'exhaustive-search', 'minimal-adjustment', 'maximal-adjustment', \n",
    "                             'efficient-adjustment', 'efficient-minimal-adjustment', 'efficient-mincost-adjustment'}\n",
    "        '''\n",
    "        self.find_adjustment_variables()\n",
    "        colliders = set()\n",
    "        non_colliders = set()\n",
    "        \n",
    "        for index, row in self.adjustment_variables.iterrows():\n",
    "            colliders.update(row['colliders'])\n",
    "            non_colliders.update(row['non_colliders'])\n",
    "        \n",
    "        # Remove X and Y from the set of nodes that we can condition on\n",
    "        for terminal in [self.node_x, self.node_y]:\n",
    "            if terminal in colliders:\n",
    "                colliders.remove(terminal)\n",
    "            if terminal in non_colliders:\n",
    "                non_colliders.remove(terminal)      \n",
    "            \n",
    "        candidate_vars = colliders.union(non_colliders)\n",
    "        \n",
    "        all_combinations = list(chain.from_iterable(combinations(candidate_vars, r) for r in range(len(candidate_vars)+1)))\n",
    "        \n",
    "        # Checking which of the combinations are valid backdoor adjustment sets\n",
    "        self.adjustment_sets = []\n",
    "        for candidate_combination in all_combinations:\n",
    "            valid = True\n",
    "            candidate_combination = set(candidate_combination)\n",
    "            for index, row in self.adjustment_variables.iterrows():\n",
    "                current_colliders = set(row['colliders'])\n",
    "                current_non_colliders = set(row['non_colliders'])\n",
    "                \n",
    "                # Conditions\n",
    "                cond1 = len(current_colliders.intersection(candidate_combination)) == 0\n",
    "                cond2 = len(current_non_colliders.intersection(candidate_combination)) > 0 \n",
    "                cond3 = len(candidate_combination.difference(non_colliders)) == 0\n",
    "                if len(current_colliders) == 0:\n",
    "                    combined_cond = cond2\n",
    "                elif len(current_non_colliders) == 0:\n",
    "                    combined_cond = cond1\n",
    "                else:\n",
    "                    combined_cond = cond1 or cond2\n",
    "\n",
    "                # Evaluate whether the candidate combination of adjustment variables meets the conditions\n",
    "                if not combined_cond or (not cond3):\n",
    "                    valid = False\n",
    "                    \n",
    "            if valid:\n",
    "                self.adjustment_sets.append(candidate_combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1db8a",
   "metadata": {},
   "source": [
    "### Backdoor criterion (DoWhy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorManagerDoWhy:\n",
    "    '''\n",
    "    This class takes care of the backdoor adjustment using DoWhy.\n",
    "    '''\n",
    "    def __init__(self, fname='backdoor_criterion_graph.gml'):\n",
    "        '''\n",
    "        :param G: the graph (a DiGraph object)\n",
    "        :param node_x: a node whose effect we are trying to predict\n",
    "        :param node_y: a node effect on which we are trying to predict\n",
    "        '''\n",
    "        self.create_data()\n",
    "        self.gml_to_string(fname)\n",
    "        self.model = CausalModel(data = self.data,\n",
    "                                 treatment='X',\n",
    "                                 outcome='Y',\n",
    "                                 graph=self.graph)\n",
    "    def create_data(self):\n",
    "        self.data = pd.DataFrame({'A':[1],'B':[1],'C':[1],'D':[1],'W':[1],'X':[1], 'Y': [1], 'Z': [1]})\n",
    "        \n",
    "    def gml_to_string(self, file):\n",
    "        gml_str = ''\n",
    "        with open(file, 'r') as file:\n",
    "            for line in file:\n",
    "                gml_str += line.rstrip()\n",
    "        self.graph = gml_str\n",
    "\n",
    "    def draw(self):\n",
    "        self.model.view_model()\n",
    "        \n",
    "    def find_adjustment_sets(self, method_name='default'): \n",
    "        '''\n",
    "        Finds backdoor adjustment sets based on the method. \n",
    "        Default method finds all the minimum-sized and maximum-sized adjustment sets, \n",
    "        see https://github.com/py-why/dowhy/blob/main/dowhy/causal_model.py\n",
    "        \n",
    "        :param method_name: {'default', 'exhaustive-search', 'minimal-adjustment', 'maximal-adjustment', \n",
    "                             'efficient-adjustment', 'efficient-minimal-adjustment', 'efficient-mincost-adjustment'}\n",
    "        '''\n",
    "        identified_estimand = self.model.identify_effect(method_name=method_name)\n",
    "        identifier = self.model.identifier\n",
    "        adjustment_sets = identifier.identify_backdoor(self.model._graph, self.model._treatment, self.model._outcome)\n",
    "        self.adjustment_sets = [back_set['backdoor_set'] for back_set in adjustment_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080bc130",
   "metadata": {},
   "source": [
    "### Frontdoor criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0440d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1eb4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13987981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf9cc01",
   "metadata": {},
   "source": [
    "### Instrumental variables\n",
    "[here](https://theeffectbook.net/ch-InstrumentalVariables.html?panelset=python-code#how-is-it-performed-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382e484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d11c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59797bed",
   "metadata": {},
   "source": [
    "### 4 Estimating Causal Effects\n",
    "\n",
    "As shown in Tutorial 4. Apply and explain different causal estimate methods (linear, inverse propensity weighting, two-stage linear regression, etc.) to the previously identified estimands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26583679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "012559ee",
   "metadata": {},
   "source": [
    "### 5 Causal Discovery\n",
    "\n",
    "As shown in Tutorials 5 and 6. Try out the two types of algorithms for learning\n",
    "causal graphs (constraint-based 10 % and score-based 10%). Explain why each method works or doesn’t and what is identifiable in terms of the causal graph.\n",
    "\n",
    "• Run a constraint-based algorithm (e.g. PC) and a score-based algorithm (e.g. GES) on your data, and report back any identifiable causal relations.\n",
    "\n",
    "• Optional: If you cannot find any identifiable causal relation or just want to test the algorithms further, simulate some data that resemble your real data (but maybe with less edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43564c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e8b066",
   "metadata": {},
   "source": [
    "### 6 Validation and Sensitivity Analysis\n",
    "\n",
    "Try out different ways to validate the results and do sensitivity analysis of the methods. \n",
    "\n",
    "• Report using some of the results of the refutation strategies implemented in DoWhy and interpret what they mean.\n",
    "\n",
    "• Optional: If your dataset includes interventional data, check that the estimated causal effects from the observational data are reflected in the interventional data.\n",
    "\n",
    "• Optional: Try experimenting with graphs in which some of the edges are dropped, and see how the results in Section 3 and 4 change.\n",
    "\n",
    "• Optional: Try relaxing some of the assumptions you discussed in the Introduction, e.g. try to see the effect on not observing a certain variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bac08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f865fe25",
   "metadata": {},
   "source": [
    "### 7 Discussion and Conclusion\n",
    "\n",
    "In this part you will discuss the results of the previous sections and explain if they do answer the causal questions you described in the Introduction. You can also elaborate on the results you observed in the validation and discuss if the assumptions you had made initially were realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70bc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dff3963",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a8fec",
   "metadata": {},
   "source": [
    "[1] Cai, J., De Janvry, A. and Sadoulet, E., 2015. Social networks and the decision to insure. American Economic Journal: Applied Economics, 7(2), pp.81-108. DOI: 10.1257/app.20130442.\n",
    "\n",
    "[2] Gretton, A., Fukumizu, K., Teo C., Song L., Schölkopf, B, and Alex Smola. 2007. A Kernel Statistical Test of Independence. Advances in Neural Information Processing Systems, https://proceedings.neurips.cc/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf.\n",
    "\n",
    "[3] OpenAI. 2023. \"Image of a Rural Chinese Village with Rice Fields and Farmers.\" DALL-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f3b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bd284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dee3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af63b5eb",
   "metadata": {},
   "source": [
    "### To-Do:\n",
    "\n",
    "* Adjust arbitrary data in handcrafter dowhy for backdoor criterion\n",
    "* Fix the environment?\n",
    "* Backdoor by hand needs more testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c497dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
