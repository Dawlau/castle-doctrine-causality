{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042a4d24",
   "metadata": {},
   "source": [
    "# Identifying the causal variables in the Castle doctrine dataset\n",
    "\n",
    "*Andrei Blahovici | Milena Kapralova | Luca Pantea | Paulius Skaisgiris*\n",
    "\n",
    "This project is part of the Causality course at the UvA during fall 2023. We work with the Castle doctrine dataset (Cheng et al., 2012) to identify whether the castle (stand-your-ground) doctrine influences the rates of violence, and what the important variables influencing this relationship are. You can also find the dataset in [this repository](https://github.com/NickCH-K/causaldata/tree/main/Python/causaldata/castle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02250d5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3732e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING:\n",
    "# The installation takes a few minutes.\n",
    "# Only run during the first time running this notebook and if you don't have these packages installed.\n",
    "# Run in terminal command line instead if it does not work.\n",
    "\n",
    "# !pip install hyppo\n",
    "# !pip install pingouin\n",
    "# !pip install conditional_independence\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7972eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import conditional_independence\n",
    "import hyppo\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pingouin as pg\n",
    "from sklearn import svm\n",
    "from IPython.display import Image, display\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# DoWhy\n",
    "import dowhy\n",
    "import dowhy.datasets\n",
    "from dowhy import CausalModel\n",
    "# from dowhy.causal_identifier import backdoor\n",
    "\n",
    "# Hide some warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configs\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924776d",
   "metadata": {},
   "source": [
    "### 1 Introduction and Motivation\n",
    "\n",
    "Introduce the datasets, the assumptions and the causal questions you are investigating.\n",
    "\n",
    "• Describe your dataset (e.g. what are the observational data and how they were collected, in case there are interventional data, also what are they and how they were collected).\n",
    "\n",
    "• Describe the causal questions you wish to answer (e.g. “we investigate the effect of X on Y”).\n",
    "\n",
    "• Describe the assumptions of your dataset (causal sufficiency, no cycles in the causal graph, positivity, etc).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "False positive use of lethal force causes a net increase in homicides relative to the counterfactual.\n",
    "\n",
    "<!-- <img src=\"us.png\" width=\"200\" height=\"500\"> -->\n",
    "\n",
    "<img src=https://d3i71xaburhd42.cloudfront.net/766441c1ab7f4390a5a8c0fa05ec9dc3cd4854d1/58-Figure1-1.png \n",
    "     align=\"center\" \n",
    "     width=\"500\" />\n",
    "     \n",
    "*Source: Shields (2016)*\n",
    "\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "We use the FBI Uniform Crime Reports Summary Part I files from 2000 to 2010. The FBI Uniform Crime Reports is a harmonized data set on eight “index” crimes collected from voluntarily participating police agencies across the country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56bbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfb515b",
   "metadata": {},
   "source": [
    "### 2 Exploratory Data Analysis\n",
    "\n",
    "As shown in Tutorial 2. \n",
    "\n",
    "• Testing correlation / dependence for the variables in the dataset and show how they are dependent.\n",
    "\n",
    "• Discuss the true causal graph of the dataset, if it’s known, and otherwise discuss a reasonable guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbace17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "875b2e7c",
   "metadata": {},
   "source": [
    "### 3 Identifying Estimands\n",
    "\n",
    "As shown in Tutorials 3 and 4. Identify possible adjustment sets by hand by using:\n",
    "\n",
    "• Backdoor criterion (most important)\n",
    "\n",
    "• Frontdoor criterion\n",
    "\n",
    "• Instrumental variables\n",
    "\n",
    "Report what happens for these methods even if they don’t apply and explain why. Also show the results you get for each of these estimands from doWhy and compare with the ones you found by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93baeb",
   "metadata": {},
   "source": [
    "#### Backdoor Criterion (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "class BackdoorManager:\n",
    "    '''\n",
    "    This class takes care of the backdoor adjustment.\n",
    "    \n",
    "    :param G: a DiGraph object\n",
    "    :param node_x: a node whose effect we are trying to predict\n",
    "    :param node_y: a node effect on which we are trying to predict\n",
    "    '''\n",
    "    def __init__(self, G, node_x, node_y):\n",
    "        self.G = G\n",
    "        self.node_x = node_x\n",
    "        self.node_y = node_y\n",
    "        self.descendants_node_x = nx.descendants(self.G, node_x) | {node_x}\n",
    "        \n",
    "    def draw(self, pos, edge_color='black'):\n",
    "        '''\n",
    "        Draws the graph given a certain position and color of the nodes.\n",
    "        '''\n",
    "        nx.draw(self.G, pos=pos, with_labels=True, node_size=500, node_color='w', edgecolors='black', edge_color=edge_color)\n",
    "        \n",
    "    def write_gml(self, fname='backdoor_criterion_graph.gml'):\n",
    "        nx.write_gml(G, fname)\n",
    "        \n",
    "    def get_all_paths(self):\n",
    "        H = self.G.to_undirected()\n",
    "        all_paths = list(nx.all_simple_paths(H, self.node_x, self.node_y))\n",
    "        return all_paths\n",
    "    \n",
    "    def get_backdoor_paths(self):\n",
    "        bd = backdoor.Backdoor(self.G, self.node_x, self.node_y)\n",
    "        all_paths = self.get_all_paths()\n",
    "        backdoor_paths = [path for path in all_paths if bd.is_backdoor(path)]\n",
    "        return backdoor_paths\n",
    "        \n",
    "    def give_coll_noncoll_on_(self, path):\n",
    "        '''\n",
    "        Finds all colliders and non-colliders on a path.\n",
    "        '''\n",
    "        colliders = np.array([])\n",
    "        non_colliders = []\n",
    "        path_len = len(path)\n",
    "        \n",
    "        # Collider\n",
    "        ## Loop through adjacent variables on the path, ignore the source and target variables as potential colliders\n",
    "        for node0, node1, node2 in zip(path[0:path_len-2], path[1:path_len-1], path[2:]):\n",
    "            if self.G.has_edge(node0, node1) and self.G.has_edge(node2, node1):\n",
    "                ## Add the collider (and all its descendants) to the list\n",
    "                colliders = np.append(colliders, list(nx.descendants(self.G,node1)) + [node1])\n",
    "        colliders = colliders.flatten()\n",
    "        \n",
    "        # Non-collider\n",
    "        non_colliders = [x for x in path[1:-1] if x not in colliders]\n",
    "\n",
    "        return colliders, non_colliders\n",
    "    \n",
    "    def find_adjustment_variables(self):\n",
    "        '''\n",
    "        Performs the backdoor criterion search.\n",
    "        '''\n",
    "        self.adjustment_variables = pd.DataFrame(columns=['path', 'colliders', 'non_colliders'])\n",
    "        paths = self.get_backdoor_paths()\n",
    "        \n",
    "        for path in paths:\n",
    "            colliders, non_colliders = self.give_coll_noncoll_on_(path)\n",
    "            self.adjustment_variables.loc[len(self.adjustment_variables.index)] = [path, colliders, non_colliders]\n",
    "    \n",
    "    def find_adjustment_sets(self, method='default'):\n",
    "        '''\n",
    "        Finds backdoor adjustment sets based on the adjustment variables and method. \n",
    "        Default method finds all the minimum-sized and maximum-sized adjustment sets, \n",
    "        \n",
    "        :param method_name: {'default', 'exhaustive-search', 'minimal-adjustment', 'maximal-adjustment', \n",
    "                             'efficient-adjustment', 'efficient-minimal-adjustment', 'efficient-mincost-adjustment'}\n",
    "        '''\n",
    "        self.find_adjustment_variables()\n",
    "        candidate_vars = set()\n",
    "        for index, row in self.adjustment_variables.iterrows():\n",
    "            candidate_vars.update(row['colliders'])\n",
    "            candidate_vars.update(row['non_colliders'])\n",
    "        candidate_vars.remove(self.node_x)\n",
    "        candidate_vars.remove(self.node_y)\n",
    "        \n",
    "        all_combinations = list(chain.from_iterable(combinations(candidate_vars, r) for r in range(len(candidate_vars)+1)))\n",
    "        \n",
    "        # Checking which of the combinations are valid backdoor adjustment sets\n",
    "        self.adjustment_sets = []\n",
    "        for candidate_combination in all_combinations:\n",
    "            valid = True\n",
    "            candidate_combination = set(candidate_combination)\n",
    "            for index, row in self.adjustment_variables.iterrows():\n",
    "                cond1 = len(set(row['colliders']).intersection(candidate_combination)) == 0\n",
    "                cond2 = len(set(row['non_colliders']).intersection(candidate_combination)) > 0 \n",
    "#                 cond3 = len(set(row['non_colliders']).intersection(candidate_combination))\n",
    "                \n",
    "                \n",
    "                if not (cond1 or cond2):\n",
    "                    valid = False\n",
    "                    break\n",
    "            if valid:\n",
    "                self.adjustment_sets.append(candidate_combination)\n",
    "                \n",
    "        print(len(self.adjustment_sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1db8a",
   "metadata": {},
   "source": [
    "#### Backdoor Criterion (DoWhy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorManagerDoWhy:\n",
    "    '''\n",
    "    This class takes care of the backdoor adjustment using DoWhy.\n",
    "    \n",
    "    :param G: a DiGraph object\n",
    "    :param node_x: a node whose effect we are trying to predict\n",
    "    :param node_y: a node effect on which we are trying to predict\n",
    "    '''\n",
    "    def __init__(self, fname='backdoor_criterion_graph.gml'):\n",
    "        self.create_data()\n",
    "        self.gml_to_string(fname)\n",
    "        self.model = CausalModel(data = self.data,\n",
    "                                 treatment='X',\n",
    "                                 outcome='Y',\n",
    "                                 graph=self.graph\n",
    "                                )\n",
    "    \n",
    "    def create_data(self):\n",
    "        self.data = pd.DataFrame({'A':[1],'B':[1],'C':[1],'D':[1],'W':[1],'X':[1], 'Y': [1], 'Z': [1]})\n",
    "        \n",
    "    def gml_to_string(self, file):\n",
    "        gml_str = ''\n",
    "        with open(file, 'r') as file:\n",
    "            for line in file:\n",
    "                gml_str += line.rstrip()\n",
    "        self.graph = gml_str\n",
    "\n",
    "    def draw(self):\n",
    "        self.model.view_model()\n",
    "        \n",
    "    def find_adjustment_sets(self, method_name='default'): \n",
    "        '''\n",
    "        Finds backdoor adjustment sets based on the method. \n",
    "        Default method finds all the minimum-sized and maximum-sized adjustment sets, \n",
    "        see https://github.com/py-why/dowhy/blob/main/dowhy/causal_model.py\n",
    "        \n",
    "        :param method_name: {'default', 'exhaustive-search', 'minimal-adjustment', 'maximal-adjustment', \n",
    "                             'efficient-adjustment', 'efficient-minimal-adjustment', 'efficient-mincost-adjustment'}\n",
    "        '''\n",
    "        identified_estimand = self.model.identify_effect(method_name=method_name)\n",
    "        identifier = self.model.identifier\n",
    "        adjustment_sets = identifier.identify_backdoor(self.model._graph, self.model._treatment, self.model._outcome)\n",
    "        self.adjustment_sets = [back_set['backdoor_set'] for back_set in adjustment_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def41968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59797bed",
   "metadata": {},
   "source": [
    "### 4 Estimating Causal Effects\n",
    "\n",
    "As shown in Tutorial 4. Apply and explain different causal estimate methods (linear, inverse propensity weighting, two-stage linear regression, etc.) to the previously identified estimands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26583679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "012559ee",
   "metadata": {},
   "source": [
    "### 5 Causal Discovery\n",
    "\n",
    "As shown in Tutorials 5 and 6. Try out the two types of algorithms for learning\n",
    "causal graphs (constraint-based 10 % and score-based 10%). Explain why each method works or doesn’t and what is identifiable in terms of the causal graph.\n",
    "\n",
    "• Run a constraint-based algorithm (e.g. PC) and a score-based algorithm (e.g. GES) on your data, and report back any identifiable causal relations.\n",
    "\n",
    "• Optional: If you cannot find any identifiable causal relation or just want to test the algorithms further, simulate some data that resemble your real data (but maybe with less edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43564c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e8b066",
   "metadata": {},
   "source": [
    "### 6 Validation and Sensitivity Analysis\n",
    "\n",
    "Try out different ways to validate the results and do sensitivity analysis of the methods. \n",
    "\n",
    "• Report using some of the results of the refutation strategies implemented in DoWhy and interpret what they mean.\n",
    "\n",
    "• Optional: If your dataset includes interventional data, check that the estimated causal effects from the observational data are reflected in the interventional data.\n",
    "\n",
    "• Optional: Try experimenting with graphs in which some of the edges are dropped, and see how the results in Section 3 and 4 change.\n",
    "\n",
    "• Optional: Try relaxing some of the assumptions you discussed in the Introduction, e.g. try to see the effect on not observing a certain variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bac08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f865fe25",
   "metadata": {},
   "source": [
    "### 7 Discussion and Conclusion\n",
    "\n",
    "In this part you will discuss the results of the previous sections and explain if they do answer the causal questions you described in the Introduction. You can also elaborate on the results you observed in the validation and discuss if the assumptions you had made initially were realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70bc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dff3963",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a8fec",
   "metadata": {},
   "source": [
    "[1] Gretton, A., Fukumizu, K., Teo C., Song L., Schölkopf, B, and Alex Smola. A Kernel Statistical Test of Independence. Advances in Neural Information Processing Systems, 2007. https://proceedings.neurips.cc/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf\n",
    "\n",
    "[2] Shields, J.C. (2016). Self-defense in the United States: A review of the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f3b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bd284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dee3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af63b5eb",
   "metadata": {},
   "source": [
    "### To-Do:\n",
    "\n",
    "* Adjust arbitrary data in handcrafter dowhy for backdoor criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c497dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
